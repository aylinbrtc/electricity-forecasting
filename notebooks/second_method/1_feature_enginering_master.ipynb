{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "148d2c11",
   "metadata": {},
   "source": [
    "# Yöntem 2: RNN için Öznitelik Mühendisliği ve Veri Hazırlığı\n",
    "\n",
    "Bu notebook'un amacı, RNN (Tekrarlayan Sinir Ağları) tabanlı zaman serisi modelleri için veri hazırlığı yapmaktır. Süreç, ham enerji ve hava durumu verilerinin yüklenmesi, hedef değişkenlerin oluşturulması, kapsamlı bir öznitelik mühendisliği ve son olarak verinin RNN modellerinin beklediği dizi (sequence) formatına dönüştürülmesini içerir.\n",
    "\n",
    "### 1. Kütüphanelerin Yüklenmesi ve Veri Setlerinin Okunması\n",
    "\n",
    "İlk adımda, veri işleme ve analiz için gerekli olan `pandas`, `numpy` gibi temel kütüphaneler ile ham veri setlerimiz (`energy_dataset.csv` ve `weather_features.csv`) projeye dahil edilir."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "142d0d51-8f91-4cf1-9abd-8dbbd5a12d1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "energy = pd.read_csv('../../data/raw/energy_dataset.csv')\n",
    "weather = pd.read_csv('../../data/raw/weather_features.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a45f4fe",
   "metadata": {},
   "source": [
    "### 2. Hedef Değişkenlerin (Target) Tanımlanması\n",
    "\n",
    "Modellerimizi eğitmek için iki farklı hedef değişkeni tanımlıyoruz. Bu değişkenleri oluştururken gelecekteki bilgiyi sızdırmamak (data leakage) kritik öneme sahiptir. Bu nedenle tüm işlemler, veriyi belirli bir zaman adımı kadar ileri kaydıran `shift()` fonksiyonu ile yapılır.\n",
    "\n",
    "- **`target_price_1h`**: Bir sonraki saatin elektrik fiyatını tahmin etmek için kullanılır.\n",
    "- **`target_price_next_day`**: Bir sonraki günün ortalama fiyatını tahmin etmek için kullanılır. Bu, 24 saat sonrası için 24 saatlik bir yuvarlanan ortalama alınarak hesaplanır.\n",
    "\n",
    "Hesaplama sonrası oluşan `NaN` (boş) değerlere sahip satırlar veri setinden çıkarılır."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f1fce298-7484-4142-955e-0b54ddfed624",
   "metadata": {},
   "outputs": [],
   "source": [
    "energy = energy.sort_values('time').reset_index(drop=True)\n",
    "energy['time'] = pd.to_datetime(energy['time'], utc=True)\n",
    "\n",
    "# 1 saat sonrası spot fiyat (her satır için geçerli)\n",
    "energy['target_price_1h'] = energy['price actual'].shift(-1)\n",
    "\n",
    "# 1 gün sonrası ortalama fiyat hedefi (sızıntısız)\n",
    "energy['target_price_next_day'] = (\n",
    "    energy['price actual']\n",
    "    .shift(-24)\n",
    "    .rolling(window=24)\n",
    "    .mean()\n",
    ")\n",
    "\n",
    "#hedefin tanımlanmadığı veriler çıkarılır\n",
    "energy = energy.dropna(subset=['target_price_1h'])\n",
    "energy = energy.dropna(subset=['target_price_next_day'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "406312ea",
   "metadata": {},
   "source": [
    "### 3. Öznitelik Mühendisliği (Feature Engineering)\n",
    "\n",
    "Modelin performansı büyük ölçüde girdi olarak kullanılan özniteliklerin kalitesine bağlıdır. Bu bölümde, ham verilerden modelin desenleri daha kolay öğrenebilmesi için çeşitli bilgilendirici öznitelikler türeteceğiz.\n",
    "\n",
    "#### 3.1. Zaman Tabanlı Öznitelikler\n",
    "\n",
    "Zaman serisindeki döngüsel (cyclical) desenleri (gün içi, hafta içi, yıl içi) yakalamak için tarih ve saat bilgisinden yeni öznitelikler oluşturulur. Sinüs/Kosinüs dönüşümleri, modelin zamanın döngüsel doğasını (örneğin, 23:00'dan sonra 00:00'ın gelmesi) sayısal olarak anlamasına yardımcı olur."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f91c037b-567e-4705-ac78-3ab87db7ca25",
   "metadata": {},
   "outputs": [],
   "source": [
    "energy['weekday'] = energy['time'].dt.weekday\n",
    "energy['month'] = energy['time'].dt.month\n",
    "energy['day'] = energy['time'].dt.day\n",
    "energy['hour'] = energy['time'].dt.hour\n",
    "energy['weekofyear'] = energy['time'].dt.isocalendar().week\n",
    "energy['year'] = energy['time'].dt.year\n",
    "\n",
    "energy['hour_sin'] = np.sin(2 * np.pi * energy['hour'] / 24)\n",
    "energy['hour_cos'] = np.cos(2 * np.pi * energy['hour'] / 24)\n",
    "\n",
    "energy['month_sin'] = np.sin(2 * np.pi * energy['month'] / 12)\n",
    "energy['month_cos'] = np.cos(2 * np.pi * energy['month'] / 12)\n",
    "\n",
    "#sadece gece 00:00 için\n",
    "energy['is_midnight'] = (energy['hour'] == 0).astype(int)\n",
    "\n",
    "energy['is_valid_for_daily_model'] = energy['target_price_next_day'].notna()\n",
    "\n",
    "#hafta sonu bilgisi\n",
    "energy['is_weekend'] = energy['weekday'].isin([5, 6]).astype(int)\n",
    "energy['is_weekday'] = (energy['weekday'] < 5).astype(int)\n",
    "\n",
    "# 1: kış, 2: ilkbahar, 3: yaz, 4: sonbahar\n",
    "def get_season(month):\n",
    "    return (month % 12 + 3) // 3\n",
    "energy['season'] = energy['month'].apply(get_season)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2c86857",
   "metadata": {},
   "source": [
    "#### 3.2. Gecikme Öznitelikleri (Lag Features)\n",
    "\n",
    "Zaman serisi verilerinde bir değer, genellikle önceki değerlerle yakından ilişkilidir (otokorelasyon). Fiyat (`price actual`) ve tüketim (`total load actual`) için geçmiş değerleri öznitelik olarak eklemek, modelin bu zamansal bağımlılığı öğrenmesini sağlar. Saatlik ve günlük modellerin ihtiyaçlarına göre farklı gecikme pencereleri (1 saat, 24 saat, 1 hafta vb.) kullanılır."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "464c368a-e2c7-400f-b8a6-d8d71f7cca64",
   "metadata": {},
   "outputs": [],
   "source": [
    "lags_common=[24,168]\n",
    "\n",
    "#saatlik çözünürlük için\n",
    "lags_hourly = [1, 2, 3]\n",
    "for lag in lags_hourly+ lags_common:\n",
    "    energy[f'price_lag_{lag}'] = energy['price actual'].shift(lag)\n",
    "    energy[f'load_lag_{lag}'] = energy['total load actual'].shift(lag)\n",
    "\n",
    "#daily tahmin için daha uzun vadeli set\n",
    "lags_daily = [48, 72, 96, 120, 144, 168]\n",
    "for lag in lags_daily+ lags_common:\n",
    "    energy[f'price_lag_{lag}'] = energy['price actual'].shift(lag)\n",
    "    energy[f'load_lag_{lag}'] = energy['total load actual'].shift(lag)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab639eea-2ca8-4ed2-a8c3-46c42c884aed",
   "metadata": {},
   "source": [
    "#### 3.3. Yuvarlanan Pencere İstatistikleri (Rolling Window Features)\n",
    "\n",
    "Geçmiş veriler üzerinden belirli bir pencere boyutunda hesaplanan istatistikler (ortalama, standart sapma vb.), serideki yerel trendleri ve oynaklığı (volatilite) yakalamak için kullanılır. Örneğin, 24 saatlik yuvarlanan ortalama, son bir günün ortalama fiyat eğilimini gösterir."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "23162ace-4592-45b6-97bb-ab822d581884",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Rolling Özellikler\n",
    "rolling_windows_hourly = [3, 6, 12]\n",
    "rolling_windows_common = [24]\n",
    "rolling_windows_daily = [48, 72, 168]\n",
    "\n",
    "#Saatlik rolling – kısa vadeli dalgalanmalar\n",
    "for window in rolling_windows_hourly:\n",
    "    energy[f'price_roll_mean_{window}'] = energy['price actual'].shift(1).rolling(window).mean()\n",
    "    energy[f'price_roll_std_{window}'] = energy['price actual'].shift(1).rolling(window).std()\n",
    "    energy[f'load_roll_mean_{window}'] = energy['total load actual'].shift(1).rolling(window).mean()\n",
    "    energy[f'load_roll_std_{window}'] = energy['total load actual'].shift(1).rolling(window).std()\n",
    "\n",
    "#Ortak rolling\n",
    "for window in rolling_windows_common:\n",
    "    energy[f'price_roll_mean_{window}'] = energy['price actual'].shift(1).rolling(window).mean()\n",
    "    energy[f'price_roll_std_{window}'] = energy['price actual'].shift(1).rolling(window).std()\n",
    "    energy[f'load_roll_mean_{window}'] = energy['total load actual'].shift(1).rolling(window).mean()\n",
    "    energy[f'load_roll_std_{window}'] = energy['total load actual'].shift(1).rolling(window).std()\n",
    "\n",
    "#Günlük model için uzun vadeli\n",
    "for window in rolling_windows_daily:\n",
    "    energy[f'price_roll_mean_{window}'] = energy['price actual'].shift(1).rolling(window).mean()\n",
    "    energy[f'price_roll_std_{window}'] = energy['price actual'].shift(1).rolling(window).std()\n",
    "    energy[f'load_roll_mean_{window}'] = energy['total load actual'].shift(1).rolling(window).mean()\n",
    "    energy[f'load_roll_std_{window}'] = energy['total load actual'].shift(1).rolling(window).std()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a34a77c",
   "metadata": {},
   "source": [
    "#### 3.4. Trend ve Momentum Öznitelikleri\n",
    "\n",
    "Bu öznitelikler, fiyat serisindeki anlık değişimleri ve yönelimleri yakalamayı hedefler.\n",
    "\n",
    "- **`price_trend_vs_mean`**: Anlık fiyatın, kendi yuvarlanan ortalamasından ne kadar saptığını gösterir.\n",
    "- **`diff` / `delta` / `pct_change`**: Fiyatın bir önceki zaman adımına göre ne kadar (mutlak veya yüzdesel) değiştiğini gösterir. Bu, momentumu ölçmek için kullanılır."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "72d4176c",
   "metadata": {},
   "outputs": [],
   "source": [
    "trend_windows = rolling_windows_hourly + rolling_windows_common + rolling_windows_daily\n",
    "for window in trend_windows:\n",
    "    mean_col = f'price_roll_mean_{window}'\n",
    "    if mean_col in energy.columns:\n",
    "        energy[f'price_trend_vs_mean_{window}'] = energy['price actual'] - energy[mean_col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "baa2e59f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#momentum özellikleri\n",
    "energy['price_diff_1']= energy['price actual'] - energy['price_lag_1']\n",
    "energy['load_diff_1']= energy['total load actual'] - energy['load_lag_1']\n",
    "\n",
    "# Fiyat delta sinyalleri\n",
    "energy['price_delta_1h_hourly'] = energy['price actual'].diff(1)\n",
    "energy['price_delta_24h'] = energy['price actual'].diff(24)\n",
    "\n",
    "# Yüzdesel değişim (ekonometrik dönüşüm)\n",
    "energy['price_pct_change_1h_hourly'] = energy['price actual'].pct_change(1)\n",
    "energy['price_pct_change_24h'] = energy['price actual'].pct_change(24)\n",
    "\n",
    "energy['price_delta_24h_daily'] = energy['price actual'].diff(24)\n",
    "energy['price_delta_168h_daily'] = energy['price actual'].diff(168)\n",
    "energy['price_pct_change_168h_daily'] = energy['price actual'].pct_change(168)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a455c164",
   "metadata": {},
   "source": [
    "#### 3.5. Bir Önceki Güne Ait İstatistikler\n",
    "\n",
    "Modelin, bir önceki günün genel fiyat davranışını (ortalama, min, maks, standart sapma) bilmesi, tahmin yaparken faydalı bir bağlam sağlar. Bu adımda, her gün için bir önceki günün özet istatistikleri hesaplanır ve mevcut veri setine eklenir."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cf6f1873",
   "metadata": {},
   "outputs": [],
   "source": [
    "energy_indexed = energy.set_index('time')\n",
    "\n",
    "# Günlük istatistikleri hesapla ve bir gün ileri kaydır\n",
    "daily_stats = energy_indexed['price actual'].resample('D').agg(['mean', 'std', 'min', 'max']).shift(1)\n",
    "\n",
    "# Sütun isimlerini daha anlaşılır yap\n",
    "daily_stats = daily_stats.add_prefix('daily_price_prev_day_')\n",
    "\n",
    "daily_stats = daily_stats.add_prefix('daily_price_prev_day_')\n",
    "\n",
    "# Saatlik veriye gün bazında merge et (her saate bir önceki günün istatistikleri eklenecek)\n",
    "# Önce energy dataframe'ine bir tarih sütunu ekleyelim\n",
    "energy['date'] = energy['time'].dt.date\n",
    "\n",
    "daily_stats.index = daily_stats.index.date\n",
    "\n",
    "# Merge işlemi\n",
    "energy = pd.merge(energy, daily_stats, left_on='date', right_index=True, how='left')\n",
    "\n",
    "energy.drop(columns=['date'], inplace=True)\n",
    "\n",
    "ffill_cols = [col for col in energy.columns if 'daily_price_prev_day' in col]\n",
    "energy[ffill_cols] = energy[ffill_cols].fillna(method='ffill')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1486825b-a303-4332-883a-c4c813ce2edd",
   "metadata": {},
   "source": [
    "#### 3.6. Dağılım ve Aykırı Değer Tespiti\n",
    "\n",
    "- **Log Dönüşümü**: Fiyat gibi sağa çarpık dağılımları normale yaklaştırmak için `log1p` dönüşümü uygulanır. Bu, bazı modellerin performansını artırabilir.\n",
    "- **Aykırı Değer Tespiti**: Fiyattaki ani ve beklenmedik sıçramaları (outlier) belirlemek için IQR (Interquartile Range) yöntemi kullanılır. Bu bilgi, modele aykırı durumları bir öznitelik olarak sunar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "33fb9daa-9d14-4f2f-9c04-3a48a3ac0110",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Log dönüşüm\n",
    "energy['log_price'] = np.log1p(energy['price actual'])\n",
    "\n",
    "# günlük için 7 gün \n",
    "q1_168 = energy['price actual'].shift(1).rolling(window=168).quantile(0.25)\n",
    "q3_168 = energy['price actual'].shift(1).rolling(window=168).quantile(0.75)\n",
    "iqr_168 = q3_168 - q1_168\n",
    "lower_168 = q1_168 - 1.5 * iqr_168\n",
    "upper_168 = q3_168 + 1.5 * iqr_168\n",
    "energy['is_price_outlier_168'] = ((energy['price actual'] < lower_168) | (energy['price actual'] > upper_168)).astype(int)\n",
    "\n",
    "#saatlik için 24 saatlik pencere\n",
    "q1_24 = energy['price actual'].shift(1).rolling(window=24).quantile(0.25)\n",
    "q3_24 = energy['price actual'].shift(1).rolling(window=24).quantile(0.75)\n",
    "iqr_24 = q3_24 - q1_24\n",
    "lower_24 = q1_24 - 1.5 * iqr_24\n",
    "upper_24 = q3_24 + 1.5 * iqr_24\n",
    "energy['is_price_outlier_24'] = ((energy['price actual'] < lower_24) | (energy['price actual'] > upper_24)).astype(int)\n",
    "\n",
    "energy['is_price_outlier'] = energy['is_price_outlier_24'] | energy['is_price_outlier_168']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18b78343-68e8-4c47-8486-7c56d32f421b",
   "metadata": {},
   "source": [
    "#### 3.7. Sinyal İşleme Tabanlı Öznitelikler (FFT ve Dalgacık)\n",
    "\n",
    "Fiyat serisini bir sinyal olarak kabul ederek, frekans domeninden öznitelikler çıkarılır.\n",
    "\n",
    "- **Hızlı Fourier Dönüşümü (FFT)**: Zaman serisindeki baskın periyodiklikleri (örn. 24 saatlik, 1 haftalık döngüler) ortaya çıkarır. `fft_mean`, `fft_std`, `fft_max` gibi istatistikler, bu frekansların gücü hakkında bilgi verir.\n",
    "- **Haar Dalgacık Dönüşümü (DWT)**: Hem zaman hem de frekans bilgisi sağlar. Bu sayede sinyaldeki ani değişimlerin veya yerel desenlerin ne zaman meydana geldiğini yakalayabilir."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e450381d-7ce3-4f94-bb55-b336f9af360b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_rolling_fft(series, window, shift=1):\n",
    "    result = {'fft_mean': [], 'fft_std': [], 'fft_max': []}\n",
    "    for i in range(len(series)):\n",
    "        if i < window + shift:\n",
    "            result['fft_mean'].append(np.nan)\n",
    "            result['fft_std'].append(np.nan)\n",
    "            result['fft_max'].append(np.nan)\n",
    "        else:\n",
    "            window_data = series.iloc[i - window - shift:i - shift]  # GELECEK VERİ KULLANILMAZ\n",
    "            fft_vals = np.fft.fft(window_data.fillna(0).values)\n",
    "            fft_mag = np.abs(fft_vals)\n",
    "            result['fft_mean'].append(fft_mag.mean())\n",
    "            result['fft_std'].append(fft_mag.std())\n",
    "            result['fft_max'].append(fft_mag.max())\n",
    "    return result\n",
    "\n",
    "# Saatlik FFT kısa süreli frekanslar için\n",
    "fft_hourly = compute_rolling_fft(energy['price actual'], window=24)\n",
    "energy['fft_mean_24'] = fft_hourly['fft_mean']\n",
    "energy['fft_std_24'] = fft_hourly['fft_std']\n",
    "energy['fft_max_24'] = fft_hourly['fft_max']\n",
    "\n",
    "# Günlük FFT daha yumuşak paternleri anlamak için\n",
    "fft_daily = compute_rolling_fft(energy['price actual'], window=72)\n",
    "energy['fft_mean_72'] = fft_daily['fft_mean']\n",
    "energy['fft_std_72'] = fft_daily['fft_std']\n",
    "energy['fft_max_72'] = fft_daily['fft_max']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91e60339",
   "metadata": {},
   "source": [
    "#### 3.8. Enerji Üretim Öznitelikleri\n",
    "\n",
    "Elektrik fiyatı, arz (üretim) ve talep (tüketim) dengesine göre belirlenir. Bu nedenle, üretim kaynaklarına dayalı öznitelikler eklenir.\n",
    "\n",
    "- **Üretim Türleri**: Fosil ve yenilenebilir kaynaklar gruplandırılır.\n",
    "- **Toplam ve Oransal Üretim**: Toplam üretim içindeki fosil ve yenilenebilir enerji payları hesaplanır.\n",
    "- **Üretim Çeşitliliği (Entropi)**: Üretim kaynaklarının ne kadar çeşitli olduğunu ölçer. Yüksek entropi, dengeli bir kaynak dağılımını ifade eder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a6696de1-5d9f-44bf-a7bd-e4e9e8060c46",
   "metadata": {},
   "outputs": [],
   "source": [
    "fossil_cols = [\n",
    "    'generation fossil gas',\n",
    "    'generation fossil hard coal',\n",
    "    'generation fossil brown coal/lignite',\n",
    "    'generation fossil oil',\n",
    "    'generation fossil oil shale',\n",
    "    'generation fossil coal-derived gas',\n",
    "    'generation fossil peat'\n",
    "]\n",
    "\n",
    "renewable_cols = [\n",
    "    'generation hydro water reservoir',\n",
    "    'generation hydro run-of-river and poundage',\n",
    "    'generation hydro pumped storage consumption',\n",
    "    'generation wind onshore',\n",
    "    'generation wind offshore',\n",
    "    'generation solar',\n",
    "    'generation biomass',\n",
    "    'generation geothermal',\n",
    "    'generation marine',\n",
    "    'generation other renewable'\n",
    "]\n",
    "\n",
    "other_cols = ['generation nuclear', 'generation waste', 'generation other']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3f210d24-cd9f-43e4-9f76-9235d8de6198",
   "metadata": {},
   "outputs": [],
   "source": [
    "#üretim verilerini 1 saat gecikmeli alıyoruz sızıntı riskine karşı\n",
    "for col in fossil_cols + renewable_cols + other_cols:\n",
    "    energy[f'{col}_lag1'] = energy[col].shift(1)\n",
    "\n",
    "lagged_cols = [f'{col}_lag1' for col in fossil_cols + renewable_cols + other_cols]\n",
    "\n",
    "# Toplam üretim\n",
    "energy['generation_total'] = energy[[f'{col}_lag1' for col in fossil_cols + renewable_cols + other_cols]].sum(axis=1)\n",
    "\n",
    "energy['fossil_total'] = energy[[f'{col}_lag1' for col in fossil_cols]].sum(axis=1)\n",
    "energy['renewable_total'] = energy[[f'{col}_lag1' for col in renewable_cols]].sum(axis=1)\n",
    "\n",
    "# Oranlar\n",
    "energy['fossil_ratio'] = energy['fossil_total'] / (energy['generation_total'] + 1e-6)\n",
    "energy['renewable_ratio'] = energy['renewable_total'] / (energy['generation_total'] + 1e-6)\n",
    "\n",
    "energy['renewable_diff_1h'] = energy['renewable_total'].diff(1)\n",
    "energy['renewable_ratio_change'] = energy['renewable_ratio'].diff(1)\n",
    "\n",
    "\n",
    "# Entropi\n",
    "def entropy(row):\n",
    "    p = row / (row.sum() + 1e-6)\n",
    "    return -(p * np.log(p + 1e-6)).sum()\n",
    "\n",
    "\n",
    "\n",
    "energy['generation_entropy'] = energy[lagged_cols].apply(entropy, axis=1)\n",
    "\n",
    "from math import log\n",
    "N = len(lagged_cols)\n",
    "energy['generation_entropy_norm'] = energy['generation_entropy'] / log(N)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9abfd18e-c4aa-437f-9ee9-f4dfd91186d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target_price_1h ile en yüksek korelasyonlu ilk 20 değişken:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "target_price_1h                                   1.000000\n",
       "price actual                                      0.966796\n",
       "log_price                                         0.940286\n",
       "price_lag_1                                       0.900505\n",
       "price_roll_mean_3                                 0.837835\n",
       "price_lag_2                                       0.821607\n",
       "target_price_next_day                             0.819545\n",
       "price_roll_mean_24                                0.803069\n",
       "price_lag_24                                      0.802385\n",
       "fft_max_24                                        0.795417\n",
       "fft_std_24                                        0.795011\n",
       "price_roll_mean_12                                0.778701\n",
       "price_roll_mean_6                                 0.776983\n",
       "price_lag_168                                     0.766842\n",
       "price_roll_mean_48                                0.763320\n",
       "price_lag_3                                       0.744609\n",
       "price_roll_mean_72                                0.744344\n",
       "fft_std_72                                        0.740641\n",
       "fft_max_72                                        0.740000\n",
       "daily_price_prev_day_daily_price_prev_day_mean    0.735298\n",
       "Name: target_price_1h, dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target_price_next_day ile en yüksek korelasyonlu ilk 20 değişken:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "target_price_next_day                             1.000000\n",
       "price_roll_mean_24                                0.867450\n",
       "price_roll_mean_12                                0.861180\n",
       "fft_max_24                                        0.861140\n",
       "fft_std_24                                        0.860927\n",
       "price_roll_mean_168                               0.850980\n",
       "price_roll_mean_48                                0.842271\n",
       "price_roll_mean_72                                0.836635\n",
       "fft_std_72                                        0.834841\n",
       "fft_max_72                                        0.833541\n",
       "price_roll_mean_6                                 0.830913\n",
       "target_price_1h                                   0.819545\n",
       "price actual                                      0.811131\n",
       "price_roll_mean_3                                 0.810606\n",
       "daily_price_prev_day_daily_price_prev_day_mean    0.806693\n",
       "price_lag_1                                       0.803046\n",
       "log_price                                         0.797094\n",
       "price_lag_2                                       0.795387\n",
       "price_lag_3                                       0.788132\n",
       "daily_price_prev_day_daily_price_prev_day_max     0.782445\n",
       "Name: target_price_next_day, dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "constant_cols = [col for col in energy.columns if energy[col].nunique() <= 1]\n",
    "\n",
    "manual_exclude_cols = [\n",
    "    'generation fossil coal-derived gas',\n",
    "    'generation fossil oil shale',\n",
    "    'generation fossil peat',\n",
    "    'generation geothermal',\n",
    "    'generation hydro pumped storage aggregated',\n",
    "    'generation marine',\n",
    "    'generation wind offshore',\n",
    "    'forecast wind offshore eday ahead'\n",
    "]\n",
    "columns_to_exclude = list(set(constant_cols + manual_exclude_cols))\n",
    "numeric_cols = energy.drop(columns=constant_cols, errors='ignore').select_dtypes(include=[np.number])\n",
    "# Sadece sayısal sütunlar içinden korelasyon analizi\n",
    "if 'target_price_1h' in numeric_cols.columns:\n",
    "    target_corr_1h = numeric_cols.corr()['target_price_1h'].sort_values(ascending=False)\n",
    "    print(\"target_price_1h ile en yüksek korelasyonlu ilk 20 değişken:\")\n",
    "    display(target_corr_1h.head(20))\n",
    "\n",
    "if 'target_price_next_day' in numeric_cols.columns:\n",
    "    target_corr_day = numeric_cols.corr()['target_price_next_day'].sort_values(ascending=False)\n",
    "    print(\"target_price_next_day ile en yüksek korelasyonlu ilk 20 değişken:\")\n",
    "    display(target_corr_day.head(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2610d82d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "numeric_cols = energy.select_dtypes(include=[np.number]).columns.tolist()\n",
    "\n",
    "#hedef sütunları hariç tutuyoruz\n",
    "exclude_targets = ['target_price_1h', 'target_price_next_day', \n",
    "                   'log_target_price_1h', 'log_target_price_next_day']\n",
    "\n",
    "features = [col for col in numeric_cols if col not in exclude_targets]\n",
    "\n",
    "#korelasyon hesaplıyoruz\n",
    "corr_1h = energy[features + ['target_price_1h']].corr()['target_price_1h'].drop('target_price_1h')\n",
    "corr_day = energy[features + ['target_price_next_day']].corr()['target_price_next_day'].drop('target_price_next_day')\n",
    "\n",
    "# DataFrame\n",
    "feature_meta = pd.DataFrame({\n",
    "    'feature_name': features,\n",
    "    'corr_1h': corr_1h,\n",
    "    'corr_next_day': corr_day\n",
    "})\n",
    "\n",
    "#kullanım etiketi\n",
    "def tag_feature(row):\n",
    "    if abs(row['corr_1h']) >= 0.75 and abs(row['corr_next_day']) >= 0.75:\n",
    "        return 'both'\n",
    "    elif abs(row['corr_1h']) >= 0.75:\n",
    "        return 'hourly_only'\n",
    "    elif abs(row['corr_next_day']) >= 0.75:\n",
    "        return 'daily_only'\n",
    "    else:\n",
    "        return 'weak'\n",
    "\n",
    "feature_meta['used_in_model'] = feature_meta.apply(tag_feature, axis=1)\n",
    "\n",
    "feature_meta.to_csv('../../data/processed/feature_metadata_2.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56713aad",
   "metadata": {},
   "source": [
    "#### 3.9. Dışsal (Exogenous) Hava Durumu Öznitelikleri\n",
    "\n",
    "Hava durumu, hem enerji tüketimini (ısıtma/soğutma) hem de yenilenebilir enerji üretimini (güneş/rüzgar) doğrudan etkiler. Bu nedenle Madrid şehri verileri, genel bir gösterge olarak kullanılarak zaman ekseninde birleştirilir ve bu verilerden yeni öznitelikler (sıcaklık farkları, yuvarlanan nem istatistikleri vb.) türetilir."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3ed82e7f-96ed-4744-8ed5-8dbdd7935b58",
   "metadata": {},
   "outputs": [],
   "source": [
    "weather['dt_iso'] = pd.to_datetime(weather['dt_iso'], utc=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "50cbaed7-d4a7-4a55-9f28-250d08f8274b",
   "metadata": {},
   "outputs": [],
   "source": [
    "madrid_weather = weather[weather['city_name'] == 'Madrid'].copy()\n",
    "madrid_weather['dt_iso'] = pd.to_datetime(madrid_weather['dt_iso'], utc=True)\n",
    "\n",
    "#prefix\n",
    "madrid_weather = madrid_weather.add_prefix('madrid_')\n",
    "madrid_weather = madrid_weather.rename(columns={'madrid_dt_iso': 'time'})\n",
    "\n",
    "energy = pd.merge(energy, madrid_weather, on='time', how='left')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "27c22bfb-993b-4b1c-9cbe-f989848724d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weather birleştirildi. Yeni shape: (36220, 169)\n"
     ]
    }
   ],
   "source": [
    "print(\"Weather birleştirildi. Yeni shape:\", energy.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "22bd600b-c399-4b81-b150-cb2f988f22f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#geçmişe göre flagli yağmur kar durumu\n",
    "energy['madrid_is_rainy'] = (energy['madrid_rain_3h'].shift(1) > 0).astype(int)\n",
    "energy['madrid_is_snowy'] = (energy['madrid_snow_3h'].shift(1) > 0).astype(int)\n",
    "\n",
    "# 3 saat öncesine göre fark sıcaklık\n",
    "energy['madrid_temp_diff_3h'] = energy['madrid_temp'] - energy['madrid_temp'].shift(3)\n",
    "\n",
    "# trend yakalama\n",
    "energy['madrid_temp_roll3_mean'] = energy['madrid_temp'].shift(1).rolling(window=3, min_periods=1).mean()\n",
    "\n",
    "# 1 haftalık nem dalgalanması\n",
    "energy['madrid_humidity_roll24_std'] = energy['madrid_humidity'].shift(1).rolling(window=24, min_periods=1).std()\n",
    "\n",
    "#wind ve pressure\n",
    "energy['madrid_wind_speed_roll6_mean'] = energy['madrid_wind_speed'].shift(1).rolling(6, min_periods=1).mean()\n",
    "energy['madrid_pressure_roll12_std'] = energy['madrid_pressure'].shift(1).rolling(12, min_periods=1).std()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7c778095-6e90-401e-96fa-803fe5f27f95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Hava durumu – target_price_1h korelasyonu:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "madrid_temp_diff_3h           0.108923\n",
       "madrid_temp_max               0.078584\n",
       "madrid_temp                   0.069565\n",
       "madrid_temp_min               0.047953\n",
       "madrid_temp_roll3_mean        0.044648\n",
       "madrid_pressure               0.019889\n",
       "madrid_pressure_roll12_std    0.011486\n",
       "madrid_humidity_roll24_std    0.007549\n",
       "madrid_weather_id             0.000874\n",
       "madrid_snow_3h                0.007969\n",
       "dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Hava durumu – target_price_next_day korelasyonu:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "madrid_pressure_roll12_std    0.031569\n",
       "madrid_weather_id             0.025445\n",
       "madrid_pressure               0.023418\n",
       "madrid_temp                   0.015032\n",
       "madrid_temp_roll3_mean        0.014897\n",
       "madrid_temp_max               0.012240\n",
       "madrid_temp_min               0.006878\n",
       "madrid_humidity_roll24_std    0.004853\n",
       "madrid_temp_diff_3h           0.001237\n",
       "madrid_humidity               0.000468\n",
       "dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "weather_cols = [\n",
    "    col for col in energy.columns \n",
    "    if col.startswith('madrid_') and np.issubdtype(energy[col].dtype, np.number)\n",
    "]\n",
    "\n",
    "if 'target_price_1h' in energy.columns:\n",
    "    print(\"\\nHava durumu – target_price_1h korelasyonu:\")\n",
    "    display(energy[weather_cols].corrwith(energy['target_price_1h']).sort_values(ascending=False).abs().head(10))\n",
    "\n",
    "if 'target_price_next_day' in energy.columns:\n",
    "    print(\"\\nHava durumu – target_price_next_day korelasyonu:\")\n",
    "    display(energy[weather_cols].corrwith(energy['target_price_next_day']).sort_values(ascending=False).abs().head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e90be7ec-c428-40a7-b74c-4252fec485e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.fftpack import fft\n",
    "import numpy as np\n",
    "\n",
    "def compute_rolling_fft_features(series, window=24, n_freq=5):\n",
    "    features = {'fft_energy': [], **{f'fft_peak_{i+1}': [] for i in range(n_freq)}}\n",
    "    shifted_series = series.shift(1)\n",
    "\n",
    "    for i in range(len(series)):\n",
    "        if i < window:\n",
    "            for k in features:\n",
    "                features[k].append(np.nan)\n",
    "        else:\n",
    "            window_data = shifted_series.iloc[i - window:i].fillna(method=\"ffill\").fillna(method=\"bfill\")\n",
    "            fft_vals = np.abs(fft(window_data.values))\n",
    "            fft_energy = np.sum(fft_vals**2)\n",
    "            top_freqs = np.sort(fft_vals)[-n_freq:]\n",
    "            for j in range(n_freq):\n",
    "                features[f'fft_peak_{j+1}'].append(top_freqs[j])\n",
    "            features['fft_energy'].append(fft_energy)\n",
    "\n",
    "    return pd.DataFrame(features, index=series.index)\n",
    "\n",
    "fft_df = compute_rolling_fft_features(energy['price actual'], window=24, n_freq=5)\n",
    "energy = energy.join(fft_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c666904a-989b-4841-be08-a9cac6c31085",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_rolling_haar_features(series, window=24):\n",
    "    def haar_transform(series):\n",
    "        n = len(series) // 2 * 2\n",
    "        approx = [(series[i] + series[i+1]) / 2 for i in range(0, n, 2)]\n",
    "        detail = [(series[i] - series[i+1]) / 2 for i in range(0, n, 2)]\n",
    "        return np.mean(approx), np.std(approx), np.mean(detail), np.std(detail)\n",
    "\n",
    "    shifted_series = series.shift(1)\n",
    "    features = {'dwt_approx_mean': [], 'dwt_approx_std': [], 'dwt_detail_mean': [], 'dwt_detail_std': []}\n",
    "\n",
    "    for i in range(len(series)):\n",
    "        if i < window:\n",
    "            for k in features:\n",
    "                features[k].append(np.nan)\n",
    "        else:\n",
    "            window_data = shifted_series.iloc[i - window:i].fillna(method=\"ffill\").fillna(method=\"bfill\").values\n",
    "            a_mean, a_std, d_mean, d_std = haar_transform(window_data)\n",
    "            features['dwt_approx_mean'].append(a_mean)\n",
    "            features['dwt_approx_std'].append(a_std)\n",
    "            features['dwt_detail_mean'].append(d_mean)\n",
    "            features['dwt_detail_std'].append(d_std)\n",
    "\n",
    "    return pd.DataFrame(features, index=series.index)\n",
    "\n",
    "dwt_df = compute_rolling_haar_features(energy['price actual'], window=24)\n",
    "energy = energy.join(dwt_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "207a5659",
   "metadata": {},
   "outputs": [],
   "source": [
    "#korelasyon tablosu ve metadata\n",
    "numeric_cols = energy.select_dtypes(include=[np.number]).columns.tolist()\n",
    "targets = ['target_price_1h', 'target_price_next_day']\n",
    "features = [col for col in numeric_cols if col not in targets + ['log_target_price_1h', 'log_target_price_next_day']]\n",
    "\n",
    "#korelasyonlar\n",
    "corr_1h = energy[features + ['target_price_1h']].corr()['target_price_1h'].drop('target_price_1h')\n",
    "corr_day = energy[features + ['target_price_next_day']].corr()['target_price_next_day'].drop('target_price_next_day')\n",
    "\n",
    "feature_meta = pd.DataFrame({\n",
    "    'feature_name': features,\n",
    "    'corr_1h': corr_1h,\n",
    "    'corr_next_day': corr_day\n",
    "})\n",
    "\n",
    "def tag_feature(row):\n",
    "    if abs(row['corr_1h']) >= 0.75 and abs(row['corr_next_day']) >= 0.75:\n",
    "        return 'both'\n",
    "    elif abs(row['corr_1h']) >= 0.75:\n",
    "        return 'hourly_only'\n",
    "    elif abs(row['corr_next_day']) >= 0.75:\n",
    "        return 'daily_only'\n",
    "    else:\n",
    "        return 'weak'\n",
    "\n",
    "feature_meta['used_in_model'] = feature_meta.apply(tag_feature, axis=1)\n",
    "\n",
    "#feature tip etiketleme\n",
    "def get_feature_type(f):\n",
    "    if 'lag' in f:\n",
    "        return 'lag'\n",
    "    elif 'roll_mean' in f:\n",
    "        return 'rolling_mean'\n",
    "    elif 'roll_std' in f:\n",
    "        return 'rolling_std'\n",
    "    elif 'fft_peak' in f or 'fft_energy' in f:\n",
    "        return 'fft'\n",
    "    elif 'dwt_' in f:\n",
    "        return 'wavelet'\n",
    "    elif 'entropy' in f:\n",
    "        return 'generation_entropy'\n",
    "    elif 'ratio' in f:\n",
    "        return 'generation_ratio'\n",
    "    elif 'madrid' in f:\n",
    "        return 'weather'\n",
    "    elif 'diff' in f or 'delta' in f:\n",
    "        return 'momentum'\n",
    "    elif 'outlier' in f:\n",
    "        return 'outlier'\n",
    "    else:\n",
    "        return 'other'\n",
    "\n",
    "feature_meta['feature_type'] = feature_meta['feature_name'].apply(get_feature_type)\n",
    "\n",
    "# Kaydet\n",
    "feature_meta.to_csv('../../data/processed/feature_metadata_2.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c05b2344",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_meta['used_in_model_1h'] = feature_meta['used_in_model'].isin(['both', 'hourly_only'])\n",
    "feature_meta['used_in_model_next_day'] = feature_meta['used_in_model'].isin(['both', 'daily_only'])\n",
    "\n",
    "feature_meta.to_csv('../../data/processed/feature_metadata_2.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "88b4c5c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "TARGET_COLS = ['target_price_1h', 'target_price_next_day']\n",
    "ID_COLS = ['time']\n",
    "\n",
    "LEAKY_SOURCE_COLS = ['price actual', 'log_price']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d2771cf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Potansiyel feature sayısı: 176\n"
     ]
    }
   ],
   "source": [
    "potential_features = [\n",
    "    col for col in energy.select_dtypes(include=np.number).columns\n",
    "    if col not in TARGET_COLS + ID_COLS + LEAKY_SOURCE_COLS\n",
    "]\n",
    "print(f\"Potansiyel feature sayısı: {len(potential_features)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "94b671d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_feature_type(col_name):\n",
    "    # Sizin oluşturduğunuz özel lag/rolling pencerelerine göre\n",
    "    if any(f in col_name for f in ['lag_1', 'lag_2', 'roll_mean_3', 'roll_std_3', 'roll_mean_6', 'roll_std_6', 'roll_mean_12', 'roll_std_12']):\n",
    "        return 'hourly_specific'\n",
    "    if any(f in col_name for f in ['lag_48', 'lag_72', 'lag_96', 'lag_120', 'lag_144', 'roll_mean_48', 'roll_std_48', 'roll_mean_72', 'roll_std_72']):\n",
    "        return 'daily_specific'\n",
    "    \n",
    "    # Genel kategoriler\n",
    "    if 'fft' in col_name: return 'fft'\n",
    "    if 'dwt' in col_name: return 'wavelet'\n",
    "    if 'madrid' in col_name: return 'weather'\n",
    "    if 'entropy' in col_name: return 'generation_entropy'\n",
    "    if 'ratio' in col_name: return 'generation_ratio'\n",
    "    if any(k in col_name for k in ['diff', 'delta', 'pct_change']): return 'momentum'\n",
    "    \n",
    "    # kalanlar ortak kabul edilir\n",
    "    return 'common'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6b94c867",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_meta = pd.DataFrame({'feature_name': potential_features})\n",
    "feature_meta['feature_type'] = feature_meta['feature_name'].apply(define_feature_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df15d40e",
   "metadata": {},
   "source": [
    "### 4. Öznitelik Seçimi ve Temizleme\n",
    "\n",
    "Türetilen çok sayıda öznitelik arasından gürültülü, gereksiz veya sızıntıya neden olabilecek olanları elemek için sistematik bir temizleme yapılır.\n",
    "\n",
    "- **Yüksek Korelasyonlu Özniteliklerin Elenmesi**: Birbiriyle %98'den daha yüksek korelasyona sahip öznitelik çiftlerinden, hedef değişkenle daha az ilişkili olanı çıkarılır. Bu, *multicollinearity* sorununu azaltır.\n",
    "- **Yüksek Oranda Boş Değer İçerenlerin Elenmesi**: %30'dan fazla `NaN` içeren öznitelikler çıkarılır.\n",
    "- **Sabit Değerli Özniteliklerin Elenmesi**: Model için hiçbir bilgi taşımayan (tek bir değere sahip) öznitelikler çıkarılır.\n",
    "- **Sızıntı Kaynağı Olanların Elenmesi**: Hedef değişkeni doğrudan içeren `price actual` gibi sütunlar öznitelik setinden çıkarılır.\n",
    "\n",
    "Bu adımlar sonunda, saatlik ve günlük modeller için kullanılacak nihai öznitelik listeleri oluşturulur."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ac923958",
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_matrix = energy[potential_features + TARGET_COLS].corr()\n",
    "cols_to_drop_redundant = set()\n",
    "threshold = 0.98\n",
    "for i in range(len(corr_matrix.columns)):\n",
    "    for j in range(i):\n",
    "        col1, col2 = corr_matrix.columns[i], corr_matrix.columns[j]\n",
    "        if col1 in potential_features and col2 in potential_features and col1 not in cols_to_drop_redundant and col2 not in cols_to_drop_redundant:\n",
    "            if abs(corr_matrix.iloc[i, j]) > threshold:\n",
    "                corr1_target = abs(corr_matrix.loc[col1, 'target_price_1h']) + abs(corr_matrix.loc[col1, 'target_price_next_day'])\n",
    "                corr2_target = abs(corr_matrix.loc[col2, 'target_price_1h']) + abs(corr_matrix.loc[col2, 'target_price_next_day'])\n",
    "                if corr1_target < corr2_target: cols_to_drop_redundant.add(col1)\n",
    "                else: cols_to_drop_redundant.add(col2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b89af24e",
   "metadata": {},
   "outputs": [],
   "source": [
    "nan_ratios = energy[potential_features].isna().mean()\n",
    "cols_to_drop_nan = set(nan_ratios[nan_ratios > 0.3].index)\n",
    "\n",
    "cols_to_drop_constant = {col for col in potential_features if energy[col].nunique() <= 1}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "bcec9df9",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_meta['drop_reason'] = None\n",
    "feature_meta.loc[feature_meta['feature_name'].isin(cols_to_drop_redundant), 'drop_reason'] = 'high_corr_redundant'\n",
    "feature_meta.loc[feature_meta['feature_name'].isin(cols_to_drop_nan), 'drop_reason'] = 'high_nan_ratio'\n",
    "feature_meta.loc[feature_meta['feature_name'].isin(cols_to_drop_constant), 'drop_reason'] = 'constant_value'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b6538a29",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_meta['is_clean'] = feature_meta['drop_reason'].isnull()\n",
    "\n",
    "hourly_types = ['hourly_specific', 'common', 'fft', 'wavelet', 'weather', 'generation_entropy', 'generation_ratio', 'momentum']\n",
    "feature_meta['used_in_model_1h'] = feature_meta['is_clean'] & feature_meta['feature_type'].isin(hourly_types)\n",
    "\n",
    "daily_types = ['daily_specific', 'common', 'fft', 'wavelet', 'weather', 'generation_entropy', 'generation_ratio', 'momentum']\n",
    "feature_meta['used_in_model_next_day'] = feature_meta['is_clean'] & feature_meta['feature_type'].isin(daily_types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a02b5026",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saatlik Model İçin Kullanılacak Güvenli Özellik Sayısı: 122\n",
      "Günlük Model İçin Kullanılacak Güvenli Özellik Sayısı: 113\n"
     ]
    }
   ],
   "source": [
    "final_features_1h = feature_meta[feature_meta['used_in_model_1h']]['feature_name'].tolist()\n",
    "final_features_next_day = feature_meta[feature_meta['used_in_model_next_day']]['feature_name'].tolist()\n",
    "\n",
    "print(f\"Saatlik Model İçin Kullanılacak Güvenli Özellik Sayısı: {len(final_features_1h)}\")\n",
    "print(f\"Günlük Model İçin Kullanılacak Güvenli Özellik Sayısı: {len(final_features_next_day)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9e0035d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Veriden toplam 43 sütun silindi.\n"
     ]
    }
   ],
   "source": [
    "full_drop_list = list(cols_to_drop_redundant | cols_to_drop_nan | cols_to_drop_constant | set(LEAKY_SOURCE_COLS))\n",
    "energy.drop(columns=full_drop_list, inplace=True, errors='ignore')\n",
    "print(f\"Veriden toplam {len(full_drop_list)} sütun silindi.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a3b75c32",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path = '../../data/processed/feature_metadata_2_cleaned.csv'\n",
    "feature_meta.to_csv(output_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f1f417aa-72b7-4b6b-aacb-1423575c05e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import joblib\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "\n",
    "SEQUENCE_LENGTH_1H = 24\n",
    "SEQUENCE_LENGTH_NEXT_DAY = 72\n",
    "\n",
    "TARGET_COL_1H = 'target_price_1h'\n",
    "TARGET_COL_NEXT_DAY = 'target_price_next_day'\n",
    "\n",
    "TRAIN_RATIO = 0.7\n",
    "VAL_RATIO = 0.15\n",
    "\n",
    "OUTPUT_DATA_PATH = '../../data/processed/'\n",
    "\n",
    "os.makedirs(OUTPUT_DATA_PATH, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77ae493a",
   "metadata": {},
   "source": [
    "### 5. RNN için Veri Setinin Son Hazırlığı\n",
    "\n",
    "Bu bölümde, temizlenmiş öznitelik seti, RNN modelinin işleyebileceği formata getirilmektedir.\n",
    "\n",
    "#### 5.1. Son Kalan Boş Değerlerin Silinmesi\n",
    "Öznitelik mühendisliği adımlarından (özellikle lag ve rolling) kaynaklanan başlangıçtaki boş değerli satırlar veri setinden tamamen temizlenir."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "63391488",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NaN içeren satırlar silindi. Yeni veri seti boyutu: (33280, 143)\n"
     ]
    }
   ],
   "source": [
    "energy.dropna(inplace=True)\n",
    "\n",
    "print(f\"NaN içeren satırlar silindi. Yeni veri seti boyutu: {energy.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0da3d8ba",
   "metadata": {},
   "source": [
    "#### 5.2. Veri Setini Kronolojik Olarak Bölme (Train-Validation-Test Split)\n",
    "\n",
    "Zaman serisi verilerinde, modelin gelecekteki veriyi görmemesi için veri setini rastgele değil, kronolojik olarak bölmek zorunludur. Verinin ilk %70'i eğitim, sonraki %15'i validasyon ve son %15'i test için ayrılmıştır."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "bdda109a-1ab3-438c-82ae-a2a6a470d63a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Veri setleri kronolojik olarak bölündü:\n",
      "  - Eğitim Seti   : 23296 satır (259 -> 25896)\n",
      "  - Validasyon Seti: 4992 satır (25897 -> 30888)\n",
      "  - Test Seti      : 4992 satır (30889 -> 36219)\n"
     ]
    }
   ],
   "source": [
    "n_samples = len(energy)\n",
    "train_split_idx = int(n_samples * TRAIN_RATIO)\n",
    "val_split_idx = int(n_samples * (TRAIN_RATIO + VAL_RATIO))\n",
    "\n",
    "# Veriyi DataFrame olarak böl\n",
    "train_df = energy.iloc[:train_split_idx].copy()\n",
    "val_df = energy.iloc[train_split_idx:val_split_idx].copy()\n",
    "test_df = energy.iloc[val_split_idx:].copy()\n",
    "\n",
    "print(\"Veri setleri kronolojik olarak bölündü:\")\n",
    "print(f\"  - Eğitim Seti   : {len(train_df)} satır ({train_df.index.min()} -> {train_df.index.max()})\")\n",
    "print(f\"  - Validasyon Seti: {len(val_df)} satır ({val_df.index.min()} -> {val_df.index.max()})\")\n",
    "print(f\"  - Test Seti      : {len(test_df)} satır ({test_df.index.min()} -> {test_df.index.max()})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ac53fb9",
   "metadata": {},
   "source": [
    "#### 5.3. Veri Ölçeklendirme (Scaling)\n",
    "\n",
    "Farklı ölçeklerdeki öznitelikler, sinir ağlarının eğitimini yavaşlatabilir veya kararsızlaştırabilir. `MinMaxScaler` kullanarak tüm öznitelikleri ve hedef değişkeni [0, 1] aralığına sıkıştırıyoruz.\n",
    "\n",
    "**Önemli Not:** Ölçekleyici (`scaler`), sadece **eğitim verisi** üzerinde eğitilir (`.fit()`) ve ardından bu eğitilmiş ölçekleyici hem eğitim, hem validasyon hem de test verisini dönüştürmek (`.transform()`) için kullanılır. Bu, validasyon ve test setlerinden eğitim sürecine bilgi sızmasını (data leakage) engeller."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "3ca4677a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hedef değişken (target) için scaler oluşturuldu ve '../../data/processed/scaler_target_next_day.joblib' adresine kaydedildi.\n",
      "Train, Val ve Test setlerindeki hedef sütunlar başarıyla ölçeklendirildi.\n"
     ]
    }
   ],
   "source": [
    "scaler_target_next_day = MinMaxScaler()\n",
    "\n",
    "scaler_target_next_day.fit(train_df[[TARGET_COL_NEXT_DAY]].values)\n",
    "\n",
    "train_df[TARGET_COL_NEXT_DAY] = scaler_target_next_day.transform(train_df[[TARGET_COL_NEXT_DAY]])\n",
    "val_df[TARGET_COL_NEXT_DAY] = scaler_target_next_day.transform(val_df[[TARGET_COL_NEXT_DAY]])\n",
    "test_df[TARGET_COL_NEXT_DAY] = scaler_target_next_day.transform(test_df[[TARGET_COL_NEXT_DAY]])\n",
    "\n",
    "scaler_target_path = os.path.join(OUTPUT_DATA_PATH, 'scaler_target_next_day.joblib')\n",
    "joblib.dump(scaler_target_next_day, scaler_target_path)\n",
    "\n",
    "print(f\"Hedef değişken (target) için scaler oluşturuldu ve '{scaler_target_path}' adresine kaydedildi.\")\n",
    "print(\"Train, Val ve Test setlerindeki hedef sütunlar başarıyla ölçeklendirildi.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb6c5e18",
   "metadata": {},
   "source": [
    "#### 5.4. Dizilerin (Sequences) Oluşturulması\n",
    "\n",
    "RNN modelleri, veriyi 3 boyutlu bir tensör olarak bekler: `(örnek_sayısı, zaman_adımı_sayısı, öznitelik_sayısı)`.\n",
    "\n",
    "Bu yapı, modele \"geçmiş `N` adımdaki özniteliklere bakarak bir sonraki adımı tahmin et\" komutunu vermemizi sağlar. `create_sequences` fonksiyonu, 2 boyutlu veri çerçevesini bu 3 boyutlu yapıya dönüştürür. Örneğin, `SEQUENCE_LENGTH_1H = 24` ise, model bir sonraki saatin fiyatını tahmin etmek için geçmiş 24 saatin verisine bakacaktır."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "1be45e49",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sequences(df, feature_cols, target_col, sequence_length):\n",
    "    X, y = [], []\n",
    "    for i in range(sequence_length, len(df)):\n",
    "        if pd.isna(df.iloc[i][target_col]):\n",
    "            continue\n",
    "        seq_x = df[feature_cols].iloc[i-sequence_length:i].values\n",
    "        seq_y = df.iloc[i][target_col]\n",
    "        X.append(seq_x)\n",
    "        y.append(seq_y)\n",
    "    return np.array(X), np.array(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6191c14f",
   "metadata": {},
   "source": [
    "### 6. Veri Setlerini Son Haline Getirme ve Kaydetme\n",
    "\n",
    "Son adım olarak, hem saatlik (`_1h`) hem de günlük (`_next_day`) modeller için ölçeklendirme ve dizilere dönüştürme işlemleri uygulanır.\n",
    "\n",
    "Oluşturulan bu işlenmiş diziler (`.npz` formatında) ve ölçekleyiciler (`.joblib` formatında) diske kaydedilir. Bu sayede, model eğitimi yapılırken bu uzun öznitelik mühendisliği sürecini tekrar çalıştırmak gerekmeyecektir."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e7f76238-9258-4006-bb46-1eba016e419f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train sequences shape: X=(23272, 24, 122), y=(23272,)\n",
      "Validation sequences shape: X=(4968, 24, 122), y=(4968,)\n",
      "Test sequences shape: X=(4968, 24, 122), y=(4968,)\n",
      "Saatlik model scaler'ı '../../data/processed/scaler_1h.joblib' adresine kaydedildi.\n"
     ]
    }
   ],
   "source": [
    "#SAATLİK MODEL\n",
    "\n",
    "\n",
    "scaler_1h = MinMaxScaler()\n",
    "\n",
    "# Scaler'ı sadece eğitim verisinin özelliklerine göre eğitiyoruz hedef sütunu ölçeklendirmeye dahil etmiyoruz.\n",
    "train_features_1h = train_df[final_features_1h]\n",
    "scaler_1h.fit(train_features_1h)\n",
    "\n",
    "# Tüm setleri eğitilmiş scaler ile dönüştür\n",
    "train_df.loc[:, final_features_1h] = scaler_1h.transform(train_features_1h)\n",
    "val_df.loc[:, final_features_1h] = scaler_1h.transform(val_df[final_features_1h])\n",
    "test_df.loc[:, final_features_1h] = scaler_1h.transform(test_df[final_features_1h])\n",
    "\n",
    "X_train_1h, y_train_1h = create_sequences(train_df, final_features_1h, TARGET_COL_1H, SEQUENCE_LENGTH_1H)\n",
    "X_val_1h, y_val_1h = create_sequences(val_df, final_features_1h, TARGET_COL_1H, SEQUENCE_LENGTH_1H)\n",
    "X_test_1h, y_test_1h = create_sequences(test_df, final_features_1h, TARGET_COL_1H, SEQUENCE_LENGTH_1H)\n",
    "\n",
    "print(f\"Train sequences shape: X={X_train_1h.shape}, y={y_train_1h.shape}\")\n",
    "print(f\"Validation sequences shape: X={X_val_1h.shape}, y={y_val_1h.shape}\")\n",
    "print(f\"Test sequences shape: X={X_test_1h.shape}, y={y_test_1h.shape}\")\n",
    "\n",
    "\n",
    "hourly_data_path = os.path.join(OUTPUT_DATA_PATH, 'hourly_model_data.npz')\n",
    "np.savez_compressed(\n",
    "    hourly_data_path,\n",
    "    X_train=X_train_1h, y_train=y_train_1h,\n",
    "    X_val=X_val_1h, y_val=y_val_1h,\n",
    "    X_test=X_test_1h, y_test=y_test_1h\n",
    ")\n",
    "\n",
    "scaler_1h_path = os.path.join(OUTPUT_DATA_PATH, 'scaler_1h.joblib')\n",
    "joblib.dump(scaler_1h, scaler_1h_path)\n",
    "print(f\"Saatlik model scaler'ı '{scaler_1h_path}' adresine kaydedildi.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "1930be8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train sequences shape: X=(23224, 72, 113), y=(23224,)\n",
      "Validation sequences shape: X=(4920, 72, 113), y=(4920,)\n",
      "Test sequences shape: X=(4920, 72, 113), y=(4920,)\n",
      "Günlük model verileri '../../data/processed/daily_model_data.npz' adresine kaydedildi.\n",
      "Günlük model scaler'ı '../../data/processed/scaler_next_day.joblib' adresine kaydedildi.\n"
     ]
    }
   ],
   "source": [
    "# GÜNLÜK MODEL\n",
    "\n",
    "scaler_next_day = MinMaxScaler()\n",
    "\n",
    "train_features_next_day = train_df[final_features_next_day]\n",
    "scaler_next_day.fit(train_features_next_day)\n",
    "\n",
    "train_df.loc[:, final_features_next_day] = scaler_next_day.transform(train_features_next_day)\n",
    "val_df.loc[:, final_features_next_day] = scaler_next_day.transform(val_df[final_features_next_day])\n",
    "test_df.loc[:, final_features_next_day] = scaler_next_day.transform(test_df[final_features_next_day])\n",
    "\n",
    "# Sequences Oluşturma\n",
    "X_train_next_day, y_train_next_day = create_sequences(train_df, final_features_next_day, TARGET_COL_NEXT_DAY, SEQUENCE_LENGTH_NEXT_DAY)\n",
    "X_val_next_day, y_val_next_day = create_sequences(val_df, final_features_next_day, TARGET_COL_NEXT_DAY, SEQUENCE_LENGTH_NEXT_DAY)\n",
    "X_test_next_day, y_test_next_day = create_sequences(test_df, final_features_next_day, TARGET_COL_NEXT_DAY, SEQUENCE_LENGTH_NEXT_DAY)\n",
    "\n",
    "print(f\"Train sequences shape: X={X_train_next_day.shape}, y={y_train_next_day.shape}\")\n",
    "print(f\"Validation sequences shape: X={X_val_next_day.shape}, y={y_val_next_day.shape}\")\n",
    "print(f\"Test sequences shape: X={X_test_next_day.shape}, y={y_test_next_day.shape}\")\n",
    "\n",
    "#Kaydetme\n",
    "daily_data_path = os.path.join(OUTPUT_DATA_PATH, 'daily_model_data.npz')\n",
    "np.savez_compressed(\n",
    "    daily_data_path,\n",
    "    X_train=X_train_next_day, y_train=y_train_next_day,\n",
    "    X_val=X_val_next_day, y_val=y_val_next_day,\n",
    "    X_test=X_test_next_day, y_test=y_test_next_day\n",
    ")\n",
    "print(f\"Günlük model verileri '{daily_data_path}' adresine kaydedildi.\")\n",
    "\n",
    "scaler_next_day_path = os.path.join(OUTPUT_DATA_PATH, 'scaler_next_day.joblib')\n",
    "joblib.dump(scaler_next_day, scaler_next_day_path)\n",
    "print(f\"Günlük model scaler'ı '{scaler_next_day_path}' adresine kaydedildi.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e41c78da",
   "metadata": {},
   "source": [
    "### 7. Son Kontrol\n",
    "\n",
    "Tüm işlemler bittikten sonra, veri setinde hala herhangi bir `NaN` değeri kalıp kalmadığı son bir kez kontrol edilir."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "242a3ddd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Öznitelik Mühendisliği Sonrası NaN Değerleri İçeren Sütunlar:\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Series([], dtype: int64)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "nan_counts = energy.isna().sum()\n",
    "\n",
    "nan_counts_with_nans = nan_counts[nan_counts > 0]\n",
    "\n",
    "sorted_nan_counts = nan_counts_with_nans.sort_values(ascending=False)\n",
    "\n",
    "print(\"Öznitelik Mühendisliği Sonrası NaN Değerleri İçeren Sütunlar:\")\n",
    "print(\"-\" * 60)\n",
    "display(sorted_nan_counts)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
